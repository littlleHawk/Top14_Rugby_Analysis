---
title: "Clean_draft"
author: "Sakina Lord"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
library(tree)
library(dplyr)
library(stringr)
library(ggplot2)
library(corrplot)
library(glmnet)
library(tidyverse)
library(cowplot)
library(mice)
library(knitr)
library(randomForest)
library(data.table)

# Global Var (so I don't have to keep looking everywhere to change it)
MICEMAXIT <-  1
```

# Load Data
```{r}
players_raw <- read.csv("clean_data/players_clean.csv")
matches_raw <- read.csv("clean_data/top14_clean.csv")
joined_raw <- read.csv("clean_data/joined.csv")
matches_long_raw <- read.csv("clean_data/top14_clean_long.csv")
```

# cleaning
```{r}
# Player data // 5 birth years are missing -> Imputation
players <- players_raw %>% 
  mutate(team = as.factor(team)) %>%
  mutate(position = as.factor(position)) %>%
  mutate(player_name = as.factor(player_name)) %>%
  mutate(birth_year = as.numeric(str_sub(birthdate, -4))) %>%
  select(-competition, -player_id, -birthdate)

# Match Data
matches <- matches_raw %>% 
  select(-date, -stadium) %>%
  mutate(season = as.factor(season),
  day = as.factor(day),
  home_team = as.factor(home_team),
  away_team = as.factor(away_team),
  winning_team = as.factor(winning_team)) %>% 
  mutate(across(matches("^home_[0-9]+$"),as.factor)) %>%
  mutate(across(matches("^away_[0-9]+$"),as.factor))

# By-Player Match data (long)
matches.long <- matches_long_raw %>% 
  select(-date, -stadium) %>%
  mutate(season = as.factor(season),
         day = as.factor(day),  
         home_team = as.factor(home_team),
         away_team = as.factor(away_team),
         winning_team = as.factor(winning_team),
         side = as.factor(side),
         player_name = as.factor(player_name))

# Joined
joined <- joined_raw %>%
  rename(position = position.y) %>%
  mutate(team = ifelse(side == "home", home_team, away_team)) %>%
  mutate(season = as.numeric(str_sub(season, end = 4)),
                             day = as.factor(day),
                             home_team = as.factor(home_team),
                             away_team = as.factor(away_team),
                             winning_team = as.factor(winning_team),
                             side = as.factor(side),
                             player_name = as.factor(player_name),
                             birth_year = as.numeric(str_sub(birthdate, -4)),
                             team = as.factor(team),
                             position = as.factor(position)) %>%
  select(-stadium, -player_id, -birthdate, -position.x, -competition, -date)


```

# Imputation
My data is missing a large amount of data in `Joined` and a few points in `Players`.
```{r}
kable(colSums(is.na(players)), caption = "Players")

kable(colSums(is.na(matches)), caption = "Matches")

kable(colSums(is.na(joined)), caption = "Joined")
```

## Players: Imputing a small set (5 missing data points) Using Random Forest
```{r}
# Create missingness indicator, drop 'conversions'
players.impute <- players %>% 
  mutate(missing_birthyr = as.integer(is.na(players$birth_year)))

# Diagnose missingness
# corrplot
players_numeric <- players.impute %>% 
  select(where(is.numeric), -missing_birthyr)
cor_matrix <- cor(players_numeric, use = "complete.obs")
corrplot(cor_matrix, type = "lower")
```
Here we see a lot of correlation:
HIGH: penalty_goals_scored & total_points_scored
      total_minutes_played & matches_started
      total_minutes_played & matches_played
      matches_started & matches_played
MED:  tries_scored & matches_started
      total_minutes_played & tries_scored, yellow_cards, birth_year
      year & birth_year
      total_points_scored & total_minutes_played, tries_scored, matches_started, matches_played, drop_goals_scored
      matches_played & tries_scored, birth_year, yellow_cards
      matches_started & tries_scored, birth_year, yellow_cards
      
```{r}
# Chi sq association test for categorical -> ISSUE player name has one per group:
  # approx may be incorrect
players_factors <- players.impute %>%
  select(where(is.factor))
PT <- chisq.test(table(players_factors$position, players_factors$team),
                 simulate.p.value = TRUE)
PN <- chisq.test(table(players_factors$position, players_factors$player_name),
                 simulate.p.value = TRUE)
TN <- chisq.test(table(players_factors$team, players_factors$player_name),
                 simulate.p.value = TRUE)

print(PT)
print(PN)
print(TN)
```
Based on these tests, player name, position, and team are highly correlated. The p-value is very low, suggesting there is evidence these values are not unrelated.

### Diagnosing Missingness
All this correlation indicates a ridge or elastic net regression approach might be best for diagnosing the type of missingness. We will build a model based on all other predictors to try and predict the birth year of a player.
```{r ridge, cache=TRUE}
# RIDGE
# remove birthyear (this is what we want to impute)
data <- players.impute %>% select(-birth_year)
x <- model.matrix(missing_birthyr~., data = data)[,-1]
y <- data$missing_birthyr

ridge_fit <-glmnet(x, y, alpha=0)
ridge_fit.cv <-cv.glmnet(x, y, alpha=0)

best_l <- ridge_fit.cv$lambda.min
cat("Best Lambda:", round(best_l, 5))

coefs_ridge <- coef(ridge_fit.cv, s = "lambda.min")

```

```{r e_fit, cache = TRUE}
elastic_fit <-glmnet(x, y, alpha=0)
elastic_fit.cv <-cv.glmnet(x, y, alpha=0)

best_l <- elastic_fit.cv$lambda.min
best_l
cat("Best Lambda:", round(best_l, 5))
coefs_elastic <- coef(elastic_fit.cv, s = "lambda.min")
```
Both Ridge and Elastic Net have small lambdas, indicating they penalize the model very little. The regression is likely very similar to its least squares counterpart. However, there is a lot of multicollinearity, so this still might be an overfit model. This is not of much concern because we want to look at how well the other predictors predict whether `birth_year` is missing.



*Looking at coefficients*
```{r}
# Extract coefficients at lambda.min
ridge_coef_df <- as.matrix(coef(ridge_fit.cv, s = "lambda.min")) %>%
  as.data.frame() %>%
  rownames_to_column("variable") %>%
  filter(variable != "(Intercept)")

elastic_coef_df <- as.matrix(coef(elastic_fit.cv, s = "lambda.min")) %>%
  as.data.frame() %>%
  rownames_to_column("variable") %>%
  filter(variable != "(Intercept)")

ridge_coef_df.cut <- ridge_coef_df %>% filter(lambda.min> 1e-5)
elastic_coef_df.cut <- elastic_coef_df %>% filter(lambda.min> 1e-5)

fig1 <- ggplot(ridge_coef_df.cut, aes(x = reorder(variable, lambda.min),
                                  y = abs(lambda.min))) +
  geom_col() +
  scale_y_log10()+
  coord_flip() + labs(title = "Ridge")

fig2 <- ggplot(elastic_coef_df.cut, aes(x = reorder(variable, lambda.min),
                                  y = abs(lambda.min))) +
  geom_col() +
  scale_y_log10()+
  coord_flip() + labs(title = "Elastic")

plot_grid(fig1, fig2)
```
Since the coefficients are very small, MAR (Missing at Random) is not plausible. The other predictors are not strong indicators of `birth_year`, so imputation will take the assumption of either MCAR (Missing Completely At Random) or MNAR (Missing Not at Random). The type of missingness helps to determine what values we choose to use to impute a value, what type of imputation, and how we interpret imputed values. In MCAR, Missingness is unrelated to any other variables meaning dropping these values from the set will not create bias in the dataset. Wih MNAR, the missingness depends on the unobserved data itself (here, that is `birth_year`) and standard imputation will create biased results. With MAR, missingness depends on observed variables, not the missing values themselves so imputation using the other variables works and standard imputation will not give biased results. The missing `birth_year` are likely MCAR as there is no motivation for why these particular players have not reported their dates of birth, so we could just drop the values without introducing bias. However, this small set might be cool to try some small imputation on.

## Imputation using package MICE: Multiple Imputation by Chained Equations
_Found using ChatGPT. Documentation here: https://cran.r-project.org/web/packages/mice/refman/mice.html _
Mice uses the Gibbs sampler to perform multiple imputation using a specified modelling method. Here we will try and compare the `pmm` (Predictive mean matching) and `rf` (Random Forest) imputation methods.

```{r players_birthyear_rf_imputation, cache=TRUE, messages=FALSE, warnings=FALSE}
imp_players_rf <- mice(players.impute,
                       m = 5,
                       method = "rf",
                       maxit = MICEMAXIT)
#Looks at how each estimated value changed over each round of imputation
imp_players_rf$imp$birth_year
```


```{r players_birthyear_pmm_imputation, messages=FALSE, warnings=FALSE, cache=TRUE}
imp_players_pmm <- mice(players.impute,
                       m = 5,
                       method = "pmm",
                       maxit = MICEMAXIT)
#Looks at how each estimated value changed over each round of imputation
imp_players_pmm$imp$birth_year
```

Taking a peek at the original versus the imputed data, the distributions are in relatively similar areas. The shapes are unlikely to be very similar due to the low number of imputed data, but they do rise in similar reigons.
```{r, cache=TRUE}
# Built by chatGPT
plot_imputation_density <- function(var_name, original_data, imp_obj, dataset = 1) {
  # Extract observed (non-missing) values
  observed <- original_data[[var_name]][!is.na(original_data[[var_name]])]
  
  # Extract imputed values for the first completed dataset by default
  if (!var_name %in% names(imp_obj$imp)) {
    stop(paste("Variable", var_name, "not found in imp_obj$imp"))
  }
  
  imputed <- imp_obj$imp[[var_name]][[dataset]]
  
  # Combine into a single data frame for plotting
  df_plot <- data.frame(
    value = c(observed, imputed),
    type = c(rep("Observed", length(observed)),
             rep("Imputed", length(imputed)))
  )
  
  # Make the density plot
  p <- ggplot(df_plot, aes(x = value, color = type)) +
    geom_density(linewidth = 1) +
    theme_minimal() +
    labs(
      x = var_name,
      y = "Density",
      title = paste("Observed vs Imputed:", var_name),
      color = "Data Type"
    )
  
  return(p)
}


# PLAYERS: BIRTH_DATE
plot_imputation_density(var_name = "birth_year",
                        original_data = players.impute,
                        imp_obj = imp_players_rf) + labs(subtitle = "Random Forest") 


plot_imputation_density(var_name = "birth_year",
                        original_data = players.impute,
                        imp_obj = imp_players_pmm) + labs(subtitle = "pmm")

```

## On Joined Data
After joining the `Players` and `Match` data, I was missing player statistics for 102,854 different player/year combinations, in addition to the extra birth years missing from `Players`. This is a much larger impute, considering this makes up 70.74% of observations Since there is such a large amount missing, I expect the imputed values to not be very accurate individually, but I hope they give a good description of the data globally.

```{r}
# Remove missig data to look at the distributions of variables 
# so we can choose the imputation method
joined_no.missing <- joined %>% na.omit()

plot(density(joined_no.missing$total_points_scored))
plot(density(joined_no.missing$matches_played))
plot(density(joined_no.missing$matches_started))
plot(density(joined_no.missing$tries_scored))
plot(density(joined_no.missing$penalty_goals_scored))
plot(density(joined_no.missing$drop_goals_scored))
plot(density(joined_no.missing$conversions))
plot(density(joined_no.missing$yellow_cards))
plot(density(joined_no.missing$red_cards))
plot(density(joined_no.missing$total_minutes_played))
plot(density(joined_no.missing$birth_year))


```

```{r fill_joined_imputation, warnings=FALSE, messages=FALSE,  cache=TRUE}
# initialize imputation
ini <- mice(joined, maxit = 0, print = FALSE)
methods <- ini$method
predictors <- ini$predictorMatrix

# Cols with missing data in joined: 
# Position is an unordered categorical variable wih more than 3 levels
methods["position"] <- "polr"
# team is also an unordered categorical, but with a lot more levels
methods["team"] <- "rf"
# pmm is reccommended for numeric variables; it preserves the distribution and does not assume linearity
# These predictors all have not normal distributions
methods["total_points_scored"] <- "pmm"
methods["matches_played"] <- "pmm"
methods["matches_started"] <- "pmm"
methods["tries_scored"] <- "pmm"
methods["penalty_goals_scored"] <- "pmm"
methods["drop_goals_scored"] <- "pmm"
methods["conversions"] <- "pmm"
methods["yellow_cards"] <- "pmm"
methods["red_cards"] <- "pmm"
methods["total_minutes_played"] <- "pmm"
# Bayesian linear regression for a numeric variable that is normally distributed
methods["birth_year"] <- "norm"

# Restrict predictors to avoid multicollinearity
predictors[] <- 0  # no predictors by default
predictors["total_points_scored", c("tries_scored","conversions","penalty_goals_scored","drop_goals_scored")] <- 1
predictors["matches_started", "matches_played"] <- 1
predictors["total_minutes_played", "matches_played"] <- 1
predictors["position", c("team","birth_year")] <- 1
predictors["team", c("position","birth_year")] <- 1
predictors["birth_year", c("season")] <- 1

# Block high-cardinality variable
predictors[, "player_name"] <- 0

imp_joined <- mice::mice(joined,
                       m = 5,
                       method  = methods,
                       predictorMatrix = predictors,
                       maxit = MICEMAXIT)
```
## Validation
Mice uses Gibbs sampling, so one method of assessing the imputations is to see if the chains converged.
```{r}
plot(imp_joined)
```

Check that relationships between variables are maintianed
```{r}
# "position"
# "team"
# 
# "total_points_scored"
# "matches_played"
# "matches_started"
# "tries_scored"
# "penalty_goals_scored"
# "drop_goals_scored"
# "conversions"
# "yellow_cards"
# "red_cards"
# "total_minutes_played"
# "birth_year"

# Imputed values are in red
xyplot(imp_joined, total_points_scored ~ matches_played)
xyplot(imp_joined, total_points_scored ~ tries_scored)
xyplot(imp_joined, total_points_scored ~ matches_started)
xyplot(imp_joined, total_points_scored ~ penalty_goals_scored)
xyplot(imp_joined, total_points_scored ~ drop_goals_scored)
xyplot(imp_joined, total_points_scored ~ conversions)
xyplot(imp_joined, total_points_scored ~ total_minutes_played)
xyplot(imp_joined, total_points_scored ~ birth_year)

xyplot(imp_joined, matches_played ~ matches_started)
xyplot(imp_joined, matches_played ~ tries_scored)
xyplot(imp_joined, matches_played ~ penalty_goals_scored)
xyplot(imp_joined, matches_played ~ drop_goals_scored)
xyplot(imp_joined, matches_played ~ conversions)
xyplot(imp_joined, matches_played ~ yellow_cards)
xyplot(imp_joined, matches_played ~ red_cards)
xyplot(imp_joined, matches_played ~ total_minutes_played)
xyplot(imp_joined, matches_played ~ birth_year)

xyplot(imp_joined, tries_scored ~ matches_started)
xyplot(imp_joined, tries_scored ~ penalty_goals_scored)
xyplot(imp_joined, tries_scored ~ drop_goals_scored)
xyplot(imp_joined, tries_scored ~ conversions)
xyplot(imp_joined, tries_scored ~ yellow_cards)
xyplot(imp_joined, tries_scored ~ red_cards)
xyplot(imp_joined, tries_scored ~ total_minutes_played)
xyplot(imp_joined, tries_scored ~ birth_year)
```


```{r}
# Correlation structure
cor_before <- joined %>%
  select(where(is.numeric)) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0))%>%
  cor(use = "pairwise.complete.obs")

cor_after  <- imp_joined %>%
  complete() %>%
  select(where(is.numeric)) %>% 
  select(where(~ sd(.x, na.rm = TRUE) != 0))%>%
  cor(use = "pairwise.complete.obs")

cor_diff <- cor_after - cor_before
par(mfrow = c(1, 3), mar = c(1, 1, 3, 1))
corrplot(cor_before, type = "lower")
corrplot(cor_after, type = "lower")
corrplot(cor_diff, type = "lower")
```
### Visuals: Diagnosing Imputed Values
Here we check that the distributions of the original and the imputed data are similar.
```{r}
# Cols with missing data in joined: 
colNames <- c("total_points_scored",
              "matches_played",
              "matches_started",
              "tries_scored",
              "penalty_goals_scored",
              "drop_goals_scored",
              "conversions",
              "yellow_cards",
              "red_cards",
              "total_minutes_played",
              "birth_year")
# Numeric
for (i in 1:length(colNames)){
  figName <- paste("fig", i, sep = "")
  # Assign from claude
  assign(figName, plot_imputation_density(var_name = colNames[i],
                        original_data = joined,
                        imp_obj = imp_joined))
}

plot_grid(fig1, fig2, fig3, fig4)
plot_grid(fig5, fig6, fig7, fig8)
plot_grid(fig9, fig10, fig11)

  # Categorical  -> Not sure how to visualise these well
# boxplot(joined$position, complete(imp_joined)$position, names = c("Observed", "Imputed"))
# boxplot(joined$team, complete(imp_joined)$team, names = c("Observed", "Imputed"))
```
The imputation looks like it preserved the distributions of all the variables. We can now fill in the rest of the `joined` dataset with the imputed values.
```{r}
joined_complete <- complete(imp_joined)

summary(joined)
summary(joined_complete)
```


# Interpretable: Single Regression Tree for the score differential
```{r}
joined_complete.lim <- joined_complete %>% select(-home_score, -away_score, -player_name)

n <- nrow(joined_complete.lim)
index <- sample(n, floor(n*.6667))
train <- joined_complete.lim %>% slice(index)

tree.joined <- tree(score_diff~., data = train)

test <- joined_complete.lim %>% slice(-index)

predict(tree.joined)

summary(tree.joined)

plot(tree.joined)
text(tree.joined, pretty = .05)
title("Score-diff Estimation Tree")
```

The most complex tree is the best one, according to cross validation
```{r}
cv.score_difftree <- cv.tree(tree.joined)
plot(cv.score_difftree$size , cv.score_difftree$dev , type = "b")
```

Evaluate the predictions using the test data. I add a pruned tree to show the largest is the best predictor.
```{r, warnings=FALSE}
yhat <- predict(tree.joined , newdata = test)
joined.test <- test$score_diff
# plot(yhat , joined.test)
# abline (0, 1)
whole.mse <- mean (( yhat - joined.test)^2)

clipped.tree <- prune.tree(tree.joined, best=8)
yhat <- predict(clipped.tree , newdata = test)
joined.test <- test$score_diff
# plot(yhat , joined.test)
# abline (0, 1)
pruned.mse <- mean (( yhat - joined.test)^2)

cbind(c("Whole Treee", "Pruned Tree"), rbind(round(whole.mse, 4), round(pruned.mse,4)))%>% kable()
```


```{r, fig.cap="Pruned Score_diff estimator Tree", warnings=FALSE}
plot(clipped.tree)
title("Pruned Tree")
text(clipped.tree, pretty = .05)
```

# Betting metric: Predictive Random Forest using Completed data

```{r}
joined_bet <- joined_complete %>% mutate(home_win = ifelse(home_score > away_score, 1, 0))
matches_bet <- matches %>% mutate(home_win = ifelse(home_score > away_score, 1, 0))
```

First, we calculate the ELO. The expected win probability of team A is given by 
$$
P_A = \frac{1}{1+10^{\frac{R_B - R_A}{400}}}
$$ 
Where $R_A$ and $R_B$ are the ratings for teams A and B, respectively. Each time a match is played, these ratings change depending on the outcome. After a match, the updated ratings are given by
$$
R'_A = R_A + K\times(S_A-P_A)
$$
Where $S_A$ is the actual outcome for team A. $S_A$ is 1 if team A wins, -0.5 for a draw, and 0 for a loss. $K$ is a sensitivity parameter. Lower values move ratings less and higher values move them more. In Top14 where there are ~26 matches per season, a $K$ value of 20-30 is reccomended, so I will use 25.

```{r}
# Code from chatGPT
elo_calc <- function(df,
                         K_regular = 20,
                         K_semi = 30,
                         K_final = 40,
                         K_barrage = 25,
                         K_access = 30,
                     # Generally accepted base Elo
                         init = 1500,
                         home_adv = 50,
                         margin_factor = 0.20) {
  # Order data by season
  df <- df %>% arrange(season, X)
  
  # Get team names
  teams <- unique(c(df$home_team, df$away_team))
  elo <- setNames(rep(init, length(teams)), teams)
  
  # Set holding variables for elo home and elo away
  elo_home <- numeric(nrow(df))
  elo_away <- numeric(nrow(df))

  # Helper to classify match type
  match_type <- function(day, winning_team, away_team, home_team) {
    d <- as.character(day)
    if (grepl("Final", d, ignore.case = TRUE)) return("final")
    if (grepl("Semi", d, ignore.case = TRUE))  return("semi")
    if(grepl("Access", d, ignore.case = TRUE)) return("access")
    if(grepl("Barrages", d, ignore.case = TRUE)) return("barrage")
    return("regular")
  }
# For each row in the dataframe
  for (i in seq_len(nrow(df))) {
    # Get team name 
    home_name <- as.character(df$home_team[i])
    away_name <- as.character(df$away_team[i])
    
    # get initial Elo
    Rh <- elo[home_name]
    Ra <- elo[away_name]

    # Determine match class
    mtype <- match_type(df$day[i], df$winning_team[i], away_name, home_name)
    K <- ifelse(mtype == "final", K_final,
                ifelse(mtype == "semi", K_semi, K_regular))

    # Home advantage (add to home Elo before computing expected probability)
    # Makes sense to add because the mean home score is higher than away 
    Rh_eff <- Rh + home_adv

    # Expected probabilities
    Ph <- 1 / (1 + 10^((Ra - Rh_eff)/400))
    Pa <- 1 - Ph

    # Actual result
    if (df$home_score[i] > df$away_score[i]) {
      Sh <- 1; Sa <- 0
    } else if (df$home_score[i] < df$away_score[i]) {
      Sh <- 0; Sa <- 1
    } else {
      Sh <- 0.5; Sa <- 0.5
    }

    # Score margin multiplier (rugby appropriate)
    margin <- abs(df$home_score[i] - df$away_score[i])
    margin_mult <- 1 + margin_factor * log(1 + margin)

    # Save baseline values for model features
    elo_home[i] <- Rh
    elo_away[i] <- Ra

    # Rating updates (use effective K)
    elo[home_name] <- Rh + (K * margin_mult) * (Sh - Ph)
    elo[away_name] <- Ra + (K * margin_mult) * (Sa - Pa)
  }

  # store as part of df
  df$elo_home <- elo_home
  df$elo_away <- elo_away

  return(df)
}
```

Apply Elo to the matches dataset.
```{r}
# Calculate and add Elo
elo_df <- elo_calc(matches_bet) %>% 
  mutate(season = as.numeric(str_sub(season, end = 4)))

# Visualise
elo_df %>% ggplot() + geom_density(aes(x=elo_home, color = home_team))+
  geom_vline(xintercept = 1500, lty = 2)
```
Add metrics based on Elo to the dataset
```{r}
# For each match, calculate Pr(home win)
elo_df <- elo_df %>% mutate(pr_home_win = (1 / (1 + 10^((elo_away - elo_home)/400))))

#how accurate is our win probability?
xyplot(elo_df$pr_home_win ~ elo_df$home_win)
```
So far, just using the Elo as a predictive win metric does not seem like a good idea. We will create more potentially predictive metrics using the player information from the `joined_complete` dataset to generate whole team statistics for each individual match lineup.

```{r}
# Get team stats for that match
team_stats <- joined_complete %>% 
  group_by(team, season, day) %>%
  summarise(avg_birth_year = mean(birth_year),
            tries_in_match = sum(tries_scored),
            penalty_goals_in_match = sum(penalty_goals_scored),
            drop_goals_in_match = sum(drop_goals_scored),
            conversions_in_match = sum(conversions),
            yellow_cards_in_match = sum(yellow_cards),
            red_cards_in_match = sum(red_cards),
            .groups = "drop")

# Using the data.table library, because this join was too big for my computer
# data.table handles joins quicker than dyplr and uses less memory
# data.table suggestion and syntax from ChatGPT
elo_df <- as.data.table(elo_df)
team_stats <- as.data.table(team_stats)

# Set key for team_stats on ("team", "season", "day")
setkey(team_stats, team, season, day)

# Join elo_df with team_stats for home team
matches_elo <- team_stats[
  elo_df,
  on = .(team = home_team, season, day),
  allow.cartesian = FALSE
]

# Add suffix "_home" to team_stats columns to differentiate from away
home_cols <- setdiff(names(team_stats), c("team", "season", "day"))
setnames(matches_elo, home_cols, paste0(home_cols, "_home"))

# Join AGAIN with team_stats for the away team
matches_elo <- team_stats[
  matches_elo,
  on = .(team = away_team, season, day),
  nomatch = 0,
  allow.cartesian = FALSE
]

# Add suffix "_away" to team_stats columns
away_cols <- setdiff(names(team_stats), c("team", "season", "day"))
away_cols <- away_cols[away_cols != "team"]   # avoid double-renaming
setnames(matches_elo, away_cols, paste0(away_cols, "_away"))
```

The new dataset includes team stats aggregating those of the lineups for both the home and away teams.
```{r}
matches_elo %>% select(1:10, 68:74) %>% 
  summary() %>% kable()
```
The data has about a 70/30 split of wins to losses which is not extremely imbalanced, but is a little skewed towards wins. This makes sense, as this is just the home team win rate. It also indicates teams are more likely to win on their home turf.
```{r}
count_table <- table(matches_elo$home_win)

cbind(c("Number of Observations", "% of Dataset"), 
      rbind(as.integer(count_table), 
      c(round((count_table[1]/(count_table[1]+count_table[2]))*100, 2), 
        round((count_table[2]/(count_table[1]+count_table[2])*100), 2)))) %>% kable()
```



### Building the predictive model
```{r}
# Remove high factor variables and values used to calculate whether the home team won
matches_elo_lim <- matches_elo %>% select(-(matches("^(home|away)_[0-9]+$")),
                                          - winning_team,
                                          - score_diff, 
                                          - home_score, 
                                          - away_score,
                                          - X,
                                          - i.team) %>% 
  mutate(home_win = as.factor(home_win))
```

### Training And Validating models
```{r kfold, cache = TRUE}
# 5x2 K-fold CV
iterations <-10
repeats = 5
num_folds = 2

# Initialize storage vectors for test errors
test.error.glm <- test.error.lasso <- test.error.ridge <- test.error.enet <- test.error.bagged_rf <- test.error.rf <- test.error.tree <-  numeric()

#Initialize values
n <- nrow(matches_elo_lim)
p <- ncol(matches_elo_lim)-1


# Fix factor levels in test to match train (Acces match only has 4 observations)
  # based on debugging from claude
matches_elo_lim <- matches_elo_lim %>%
  filter(day != "Access Match")

for(i in 1:iterations){
  for (j in 1:repeats){
    folds <- caret::createFolds(matches_elo_lim$home_win, k= num_folds)
    for (k in 1:num_folds){
        # Split train and test
        index <- folds[[k]]
        train <- matches_elo_lim %>% slice(-index)
        test <- matches_elo_lim %>% slice(index)
        
        # Build models (Comparing a single tree, glm, and bagged random forest)
        glm.elo <- glm(home_win~., data = train, family = "binomial")
        
        # trees!
        tree.elo <- tree(home_win~., data = train)
        forest.elo.bagged <- randomForest(home_win~., data = train, 
                                   mtry= p,
                                   importance = TRUE,
                                   ntree = 1000)
        forest.elo <- randomForest(home_win~., data = train,
                                   mtry = floor(sqrt(p)),
                                   importance = TRUE,
                                   ntree = 1000)
        
        # Try to improve using penalization: glmnet
            # predictors matrix
        x <- model.matrix(home_win ~ ., train)[,-1]
          # Response vector
        y <- train$home_win
          # train models
        lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")
        ridge_model <- cv.glmnet(x, y, alpha = 0, family = "binomial")
        enet_model <- cv.glmnet(x, y, alpha = 0.5, family = "binomial")
  
        # Get accuracies
          #GLM
        yhat.glm.probs <- predict(glm.elo, newdata = test)
        yhat.glm <- ifelse(yhat.glm.probs > 0.5, 1, 0)
        test.error.glm <-c(test.error.glm, mean(yhat.glm == test$home_win))
        
          # bagged rf
        yhat.bag <- predict(forest.elo.bagged , newdata = test, type = "class")
        test.error.bagged_rf <-c(test.error.bagged_rf, mean (yhat.bag == test$home_win))
        
          # Regular rf
        yhat.rf<- predict(forest.elo , newdata = test, type = "class")
        test.error.rf <-c(test.error.rf, mean (yhat.rf == test$home_win))
        
          # Single Tree
        yhat <- predict(tree.elo , newdata = test, type = "class")
        test.error.tree <- c(test.error.tree, mean (yhat == test$home_win))
  
          # glmnet
        x_test <- model.matrix(home_win ~ ., test)[,-1]
        
        yhat_prob_lasso <- predict(lasso_model, newx = x_test, type = "response", s = "lambda.min")
        yhat_prob_ridge <- predict(ridge_model, newx = x_test, type = "response", s = "lambda.min")
        yhat_prob_enet <- predict(enet_model, newx = x_test, type = "response", s = "lambda.min")
        
        yhat_prob_lasso <- ifelse(yhat_prob_lasso > 0.5, 1, 0)
        yhat_prob_ridge <- ifelse(yhat_prob_ridge > 0.5, 1, 0)
        yhat_prob_enet <- ifelse(yhat_prob_enet > 0.5, 1, 0)
        
        test.error.lasso <- c(test.error.lasso, mean(yhat_prob_lasso == test$home_win))
        test.error.ridge <- c(test.error.ridge, mean(yhat_prob_ridge == test$home_win))
        test.error.enet <- c(test.error.enet, mean(yhat_prob_enet == test$home_win))
    }
  }
}


```

Taking a look at the models
```{r}
summary(glm.elo)

summary(tree.elo)
plot(tree.elo)
text(tree.elo, pretty = .05)
title("Home Win Estimation Tree")

summary(forest.elo)
forest.elo

summary(forest.elo.bagged)
forest.elo.bagged
```

*Varaiable Importance*
```{r}
varImpPlot(forest.elo.bagged)
```
It looks like `pr_home_win` and `day` are the most important predictors, followed by `elo_home`, `elo_away`, and `team`. Elo seems to be a helpful predictor in these models.
```{r}
varImpPlot(forest.elo)
```
The regular random forest has the same set of most important variables as the bagged model. For building these models, I could have stripped away all the unimportant variables to limit the noise in predictions or just make a cleaner model.

*Pruning the Single Tree*
```{r, warnings=FALSE}
# Cross validation indicates a tree of size 2 is the preferred model
cv.elo.tree <- cv.tree(tree.elo)
plot(cv.elo.tree$size , cv.elo.tree$dev , type = "b")

# So prune tree and get accuracy
tree.elo.clipped <- prune.tree(tree.elo, best = 2)
yhat <- predict(tree.elo.clipped , newdata = test, type = "class")
pruned.tree.accuracy <- mean(yhat == test$home_win)

plot(tree.elo.clipped)
text(tree.elo.clipped, pretty = .05)
```


```{r}
# calculate Elo accuracy
yhat.elo <- ifelse(elo_df$pr_home_win > 0.5, 1, 0)
elo.accuracy <- mean(yhat.elo == elo_df$home_win)

# Build into pretty table
accuracies_df <- cbind(c("Whole Single Tree", "Pruned Single Tree", "Random Forest", 
        "Bagged Random Forest", "GLM", "LASSO", "Ridge", "Elastic Net", "Elo"), 
      rbind(round(mean(test.error.tree), 5), 
            round(pruned.tree.accuracy, 5),
            round(mean(test.error.rf), 5),
            round(mean(test.error.bagged_rf), 5),
            round(mean(test.error.glm), 5),
            round(mean(test.error.lasso), 5),
            round(mean(test.error.ridge), 5),
            round(mean(test.error.enet), 5),
            round(elo.accuracy, 5)
            ),
      rbind(round(sd(test.error.tree), 5), 
            round(pruned.tree.accuracy, 5),
            round(sd(test.error.rf), 5),
            round(sd(test.error.bagged_rf), 5),
            round(sd(test.error.glm), 5),
            round(sd(test.error.lasso), 5),
            round(sd(test.error.ridge), 5),
            round(sd(test.error.enet), 5),
            round(elo.accuracy, 5)
            )) %>% as.data.frame()

accuracies_df_pretty <- accuracies_df %>% 
  rename(Model = V1, Accuracy = V2, s.d. = V3 ) %>% 
  mutate(Model = as.factor(Model),
         Accuracy = as.numeric(Accuracy),
         s.d.= as.numeric(s.d.)) %>%
  arrange(desc(Accuracy)) 

accuracies_df_pretty %>% select(Model, Accuracy) %>% kable()
```
```{r}
accuracies_df_pretty %>% filter(Accuracy != s.d.)%>%
  mutate(Model = reorder(Model, Accuracy)) %>%
  ggplot(aes(x = Model, y = Accuracy)) +
  geom_boxplot(aes(color = Model)) +
  geom_errorbar(aes(ymin = Accuracy - s.d., ymax = Accuracy + s.d.), width = 0.3)
```

All of these models do better than the raw Elo score, with the Random forest and Elastic Net models being the most effective. The random forest could be improved with boosing or cross validation to tune hyperparameters such as the number of predictors used, `mtry`, and umber of trees in the forest, `ntrees`. Elastic Net is already cross validated to use the best lambda.